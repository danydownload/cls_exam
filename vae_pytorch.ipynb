{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TKa5SeZeymW",
        "outputId": "f55a060f-90c0-4d3b-bd11-9b3dfff1cb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Epoch: 1 2022-11-27 10:08:10.046737\n",
            "0 391 Loss: 2481.576172\n",
            "50 391 Loss: 2045.660350\n",
            "100 391 Loss: 1986.023481\n",
            "150 391 Loss: 1955.800401\n",
            "200 391 Loss: 1937.612117\n",
            "250 391 Loss: 1923.644343\n",
            "300 391 Loss: 1913.300526\n",
            "350 391 Loss: 1904.520941\n",
            "156 157 ValLoss: 1847.228898\n",
            "\n",
            "Epoch: 2 2022-11-27 10:08:22.800286\n",
            "0 391 Loss: 1828.051636\n",
            "50 391 Loss: 1844.120323\n",
            "100 391 Loss: 1842.144925\n",
            "150 391 Loss: 1841.764691\n",
            "200 391 Loss: 1840.764325\n",
            "250 391 Loss: 1840.261264\n",
            "300 391 Loss: 1839.402162\n",
            "350 391 Loss: 1839.445781\n",
            "156 157 ValLoss: 1836.342785\n",
            "\n",
            "Epoch: 3 2022-11-27 10:08:35.489072\n",
            "0 391 Loss: 1831.519409\n",
            "50 391 Loss: 1835.848544\n",
            "100 391 Loss: 1834.260244\n",
            "150 391 Loss: 1833.861302\n",
            "200 391 Loss: 1832.977820\n",
            "250 391 Loss: 1832.575517\n",
            "300 391 Loss: 1832.805284\n",
            "350 391 Loss: 1832.973369\n",
            "156 157 ValLoss: 1831.783450\n",
            "\n",
            "Epoch: 4 2022-11-27 10:08:48.063798\n",
            "0 391 Loss: 1801.735229\n",
            "50 391 Loss: 1830.772774\n",
            "100 391 Loss: 1833.393537\n",
            "150 391 Loss: 1831.448954\n",
            "200 391 Loss: 1830.714761\n",
            "250 391 Loss: 1830.719612\n",
            "300 391 Loss: 1829.991190\n",
            "350 391 Loss: 1830.101647\n",
            "156 157 ValLoss: 1830.742179\n",
            "\n",
            "Epoch: 5 2022-11-27 10:09:00.667737\n",
            "0 391 Loss: 1824.989502\n",
            "50 391 Loss: 1824.772348\n",
            "100 391 Loss: 1826.646638\n",
            "150 391 Loss: 1827.906645\n",
            "200 391 Loss: 1828.152485\n",
            "250 391 Loss: 1827.708953\n",
            "300 391 Loss: 1827.612486\n",
            "350 391 Loss: 1827.714293\n",
            "156 157 ValLoss: 1828.300401\n",
            "\n",
            "Epoch: 6 2022-11-27 10:09:13.189767\n",
            "0 391 Loss: 1837.976318\n",
            "50 391 Loss: 1827.706177\n",
            "100 391 Loss: 1826.793901\n",
            "150 391 Loss: 1826.443977\n",
            "200 391 Loss: 1826.642749\n",
            "250 391 Loss: 1826.403892\n",
            "300 391 Loss: 1826.181408\n",
            "350 391 Loss: 1825.991094\n",
            "156 157 ValLoss: 1828.082764\n",
            "\n",
            "Epoch: 7 2022-11-27 10:09:25.806963\n",
            "0 391 Loss: 1830.002808\n",
            "50 391 Loss: 1826.198575\n",
            "100 391 Loss: 1823.908161\n",
            "150 391 Loss: 1824.314423\n",
            "200 391 Loss: 1824.323284\n",
            "250 391 Loss: 1824.142325\n",
            "300 391 Loss: 1824.964559\n",
            "350 391 Loss: 1824.896049\n",
            "156 157 ValLoss: 1826.355837\n",
            "\n",
            "Epoch: 8 2022-11-27 10:09:38.417264\n",
            "0 391 Loss: 1810.239136\n",
            "50 391 Loss: 1824.237295\n",
            "100 391 Loss: 1824.250770\n",
            "150 391 Loss: 1825.298343\n",
            "200 391 Loss: 1824.603630\n",
            "250 391 Loss: 1824.960833\n",
            "300 391 Loss: 1824.548182\n",
            "350 391 Loss: 1824.268719\n",
            "156 157 ValLoss: 1826.470424\n",
            "\n",
            "Epoch: 9 2022-11-27 10:09:51.016318\n",
            "0 391 Loss: 1802.038940\n",
            "50 391 Loss: 1822.040535\n",
            "100 391 Loss: 1823.618954\n",
            "150 391 Loss: 1824.131997\n",
            "200 391 Loss: 1824.385686\n",
            "250 391 Loss: 1824.391429\n",
            "300 391 Loss: 1823.962787\n",
            "350 391 Loss: 1823.880151\n",
            "156 157 ValLoss: 1825.548371\n",
            "\n",
            "Epoch: 10 2022-11-27 10:10:03.604811\n",
            "0 391 Loss: 1844.562622\n",
            "50 391 Loss: 1823.032827\n",
            "100 391 Loss: 1822.548321\n",
            "150 391 Loss: 1822.443931\n",
            "200 391 Loss: 1822.672719\n",
            "250 391 Loss: 1823.432532\n",
            "300 391 Loss: 1823.141091\n",
            "350 391 Loss: 1823.383685\n",
            "156 157 ValLoss: 1825.038981\n",
            "\n",
            "Epoch: 11 2022-11-27 10:10:16.199641\n",
            "0 391 Loss: 1792.324341\n",
            "50 391 Loss: 1824.714872\n",
            "100 391 Loss: 1824.399615\n",
            "150 391 Loss: 1824.647543\n",
            "200 391 Loss: 1825.753949\n",
            "250 391 Loss: 1825.427645\n",
            "300 391 Loss: 1824.155406\n",
            "350 391 Loss: 1822.992660\n",
            "156 157 ValLoss: 1824.240092\n",
            "\n",
            "Epoch: 12 2022-11-27 10:10:28.752950\n",
            "0 391 Loss: 1815.325684\n",
            "50 391 Loss: 1823.342232\n",
            "100 391 Loss: 1823.543707\n",
            "150 391 Loss: 1823.739356\n",
            "200 391 Loss: 1823.073696\n",
            "250 391 Loss: 1822.996558\n",
            "300 391 Loss: 1822.846751\n",
            "350 391 Loss: 1822.273549\n",
            "156 157 ValLoss: 1823.757361\n",
            "\n",
            "Epoch: 13 2022-11-27 10:10:41.274674\n",
            "0 391 Loss: 1825.057739\n",
            "50 391 Loss: 1822.406772\n",
            "100 391 Loss: 1820.870608\n",
            "150 391 Loss: 1820.498217\n",
            "200 391 Loss: 1821.007202\n",
            "250 391 Loss: 1821.745663\n",
            "300 391 Loss: 1822.044406\n",
            "350 391 Loss: 1822.397254\n",
            "156 157 ValLoss: 1822.982779\n",
            "\n",
            "Epoch: 14 2022-11-27 10:10:53.921553\n",
            "0 391 Loss: 1815.531738\n",
            "50 391 Loss: 1826.272815\n",
            "100 391 Loss: 1823.130861\n",
            "150 391 Loss: 1821.378241\n",
            "200 391 Loss: 1821.876609\n",
            "250 391 Loss: 1822.025833\n",
            "300 391 Loss: 1820.575084\n",
            "350 391 Loss: 1821.233870\n",
            "156 157 ValLoss: 1823.034896\n",
            "\n",
            "Epoch: 15 2022-11-27 10:11:06.540837\n",
            "0 391 Loss: 1825.157593\n",
            "50 391 Loss: 1821.753394\n",
            "100 391 Loss: 1823.127298\n",
            "150 391 Loss: 1821.290024\n",
            "200 391 Loss: 1820.756314\n",
            "250 391 Loss: 1820.802604\n",
            "300 391 Loss: 1820.773580\n",
            "350 391 Loss: 1821.224339\n",
            "156 157 ValLoss: 1823.261184\n",
            "\n",
            "Epoch: 16 2022-11-27 10:11:19.201577\n",
            "0 391 Loss: 1826.009766\n",
            "50 391 Loss: 1819.593714\n",
            "100 391 Loss: 1821.070940\n",
            "150 391 Loss: 1820.250480\n",
            "200 391 Loss: 1820.258664\n",
            "250 391 Loss: 1820.091884\n",
            "300 391 Loss: 1820.614388\n",
            "350 391 Loss: 1820.814045\n",
            "156 157 ValLoss: 1822.697091\n",
            "\n",
            "Epoch: 17 2022-11-27 10:11:31.771070\n",
            "0 391 Loss: 1818.411011\n",
            "50 391 Loss: 1816.141582\n",
            "100 391 Loss: 1818.377608\n",
            "150 391 Loss: 1819.127771\n",
            "200 391 Loss: 1819.892698\n",
            "250 391 Loss: 1820.321196\n",
            "300 391 Loss: 1820.242512\n",
            "350 391 Loss: 1820.583397\n",
            "156 157 ValLoss: 1822.482413\n",
            "\n",
            "Epoch: 18 2022-11-27 10:11:44.280725\n",
            "0 391 Loss: 1821.844849\n",
            "50 391 Loss: 1826.221045\n",
            "100 391 Loss: 1820.289038\n",
            "150 391 Loss: 1820.126070\n",
            "200 391 Loss: 1821.219264\n",
            "250 391 Loss: 1821.786374\n",
            "300 391 Loss: 1821.730910\n",
            "350 391 Loss: 1820.854544\n",
            "156 157 ValLoss: 1823.135608\n",
            "\n",
            "Epoch: 19 2022-11-27 10:11:56.916370\n",
            "0 391 Loss: 1799.418823\n",
            "50 391 Loss: 1820.624117\n",
            "100 391 Loss: 1818.692064\n",
            "150 391 Loss: 1819.222700\n",
            "200 391 Loss: 1819.665199\n",
            "250 391 Loss: 1819.783873\n",
            "300 391 Loss: 1819.696897\n",
            "350 391 Loss: 1820.173138\n",
            "156 157 ValLoss: 1822.555834\n",
            "\n",
            "Epoch: 20 2022-11-27 10:12:09.449041\n",
            "0 391 Loss: 1822.209595\n",
            "50 391 Loss: 1820.148957\n",
            "100 391 Loss: 1821.187237\n",
            "150 391 Loss: 1821.771458\n",
            "200 391 Loss: 1820.675549\n",
            "250 391 Loss: 1820.709437\n",
            "300 391 Loss: 1820.868234\n",
            "350 391 Loss: 1820.645035\n",
            "156 157 ValLoss: 1822.068793\n",
            "\n",
            "Epoch: 21 2022-11-27 10:12:22.011704\n",
            "0 391 Loss: 1782.441772\n",
            "50 391 Loss: 1817.436713\n",
            "100 391 Loss: 1818.643529\n",
            "150 391 Loss: 1820.229070\n",
            "200 391 Loss: 1820.543727\n",
            "250 391 Loss: 1820.434304\n",
            "300 391 Loss: 1820.051665\n",
            "350 391 Loss: 1820.187454\n",
            "156 157 ValLoss: 1821.546025\n",
            "\n",
            "Epoch: 22 2022-11-27 10:12:34.556754\n",
            "0 391 Loss: 1834.893066\n",
            "50 391 Loss: 1822.967927\n",
            "100 391 Loss: 1820.087075\n",
            "150 391 Loss: 1819.010181\n",
            "200 391 Loss: 1820.352656\n",
            "250 391 Loss: 1820.545367\n",
            "300 391 Loss: 1820.096772\n",
            "350 391 Loss: 1820.193356\n",
            "156 157 ValLoss: 1821.770220\n",
            "\n",
            "Epoch: 23 2022-11-27 10:12:47.105112\n",
            "0 391 Loss: 1839.315674\n",
            "50 391 Loss: 1819.859667\n",
            "100 391 Loss: 1821.506310\n",
            "150 391 Loss: 1818.177352\n",
            "200 391 Loss: 1818.758420\n",
            "250 391 Loss: 1818.451496\n",
            "300 391 Loss: 1818.710278\n",
            "350 391 Loss: 1818.851012\n",
            "156 157 ValLoss: 1822.116444\n",
            "\n",
            "Epoch: 24 2022-11-27 10:12:59.744846\n",
            "0 391 Loss: 1809.573853\n",
            "50 391 Loss: 1819.963661\n",
            "100 391 Loss: 1819.027520\n",
            "150 391 Loss: 1818.302092\n",
            "200 391 Loss: 1818.434059\n",
            "250 391 Loss: 1819.003069\n",
            "300 391 Loss: 1820.065980\n",
            "350 391 Loss: 1819.726165\n",
            "156 157 ValLoss: 1822.155290\n",
            "\n",
            "Epoch: 25 2022-11-27 10:13:12.279492\n",
            "0 391 Loss: 1791.968018\n",
            "50 391 Loss: 1820.751326\n",
            "100 391 Loss: 1818.982355\n",
            "150 391 Loss: 1819.392936\n",
            "200 391 Loss: 1819.341269\n",
            "250 391 Loss: 1819.175181\n",
            "300 391 Loss: 1819.318374\n",
            "350 391 Loss: 1819.530082\n",
            "156 157 ValLoss: 1821.652779\n",
            "\n",
            "Epoch: 26 2022-11-27 10:13:24.871079\n",
            "0 391 Loss: 1816.460205\n",
            "50 391 Loss: 1820.606046\n",
            "100 391 Loss: 1819.576681\n",
            "150 391 Loss: 1819.177428\n",
            "200 391 Loss: 1820.239948\n",
            "250 391 Loss: 1819.689647\n",
            "300 391 Loss: 1819.219667\n",
            "350 391 Loss: 1819.970389\n",
            "156 157 ValLoss: 1821.387876\n",
            "\n",
            "Epoch: 27 2022-11-27 10:13:37.441359\n",
            "0 391 Loss: 1842.767700\n",
            "50 391 Loss: 1819.391001\n",
            "100 391 Loss: 1820.110906\n",
            "150 391 Loss: 1819.821405\n",
            "200 391 Loss: 1820.596195\n",
            "250 391 Loss: 1820.262679\n",
            "300 391 Loss: 1819.510646\n",
            "350 391 Loss: 1819.067476\n",
            "156 157 ValLoss: 1821.940526\n",
            "\n",
            "Epoch: 28 2022-11-27 10:13:50.008938\n",
            "0 391 Loss: 1796.114624\n",
            "50 391 Loss: 1822.628827\n",
            "100 391 Loss: 1819.917091\n",
            "150 391 Loss: 1820.817861\n",
            "200 391 Loss: 1819.662196\n",
            "250 391 Loss: 1819.014125\n",
            "300 391 Loss: 1819.160881\n",
            "350 391 Loss: 1819.178019\n",
            "156 157 ValLoss: 1821.533215\n",
            "\n",
            "Epoch: 29 2022-11-27 10:14:02.627408\n",
            "0 391 Loss: 1832.854370\n",
            "50 391 Loss: 1815.068986\n",
            "100 391 Loss: 1815.762892\n",
            "150 391 Loss: 1815.737009\n",
            "200 391 Loss: 1817.115088\n",
            "250 391 Loss: 1817.187820\n",
            "300 391 Loss: 1817.876956\n",
            "350 391 Loss: 1818.840411\n",
            "156 157 ValLoss: 1820.781934\n",
            "\n",
            "Epoch: 30 2022-11-27 10:14:15.198660\n",
            "0 391 Loss: 1788.038330\n",
            "50 391 Loss: 1820.158773\n",
            "100 391 Loss: 1820.383985\n",
            "150 391 Loss: 1818.596634\n",
            "200 391 Loss: 1819.500991\n",
            "250 391 Loss: 1818.496530\n",
            "300 391 Loss: 1819.593453\n",
            "350 391 Loss: 1819.360622\n",
            "156 157 ValLoss: 1821.681727\n",
            "\n",
            "Epoch: 31 2022-11-27 10:14:27.748721\n",
            "0 391 Loss: 1824.273193\n",
            "50 391 Loss: 1819.997616\n",
            "100 391 Loss: 1818.837518\n",
            "150 391 Loss: 1819.167674\n",
            "200 391 Loss: 1818.509840\n",
            "250 391 Loss: 1819.489546\n",
            "300 391 Loss: 1819.168743\n",
            "350 391 Loss: 1818.772711\n",
            "156 157 ValLoss: 1820.964654\n",
            "\n",
            "Epoch: 32 2022-11-27 10:14:40.280991\n",
            "0 391 Loss: 1839.815063\n",
            "50 391 Loss: 1819.582850\n",
            "100 391 Loss: 1818.108498\n",
            "150 391 Loss: 1817.757813\n",
            "200 391 Loss: 1817.640660\n",
            "250 391 Loss: 1817.692191\n",
            "300 391 Loss: 1818.190558\n",
            "350 391 Loss: 1818.381072\n",
            "156 157 ValLoss: 1820.724441\n",
            "\n",
            "Epoch: 33 2022-11-27 10:14:52.822109\n",
            "0 391 Loss: 1822.592773\n",
            "50 391 Loss: 1821.705272\n",
            "100 391 Loss: 1819.583150\n",
            "150 391 Loss: 1818.295469\n",
            "200 391 Loss: 1818.555569\n",
            "250 391 Loss: 1817.886927\n",
            "300 391 Loss: 1817.575069\n",
            "350 391 Loss: 1817.587927\n",
            "156 157 ValLoss: 1820.504525\n",
            "\n",
            "Epoch: 34 2022-11-27 10:15:05.383996\n",
            "0 391 Loss: 1825.203857\n",
            "50 391 Loss: 1816.883866\n",
            "100 391 Loss: 1817.947573\n",
            "150 391 Loss: 1818.228512\n",
            "200 391 Loss: 1818.635651\n",
            "250 391 Loss: 1818.715302\n",
            "300 391 Loss: 1817.953438\n",
            "350 391 Loss: 1818.355619\n",
            "156 157 ValLoss: 1820.678068\n",
            "\n",
            "Epoch: 35 2022-11-27 10:15:17.952630\n",
            "0 391 Loss: 1847.277100\n",
            "50 391 Loss: 1823.271657\n",
            "100 391 Loss: 1822.239058\n",
            "150 391 Loss: 1819.443205\n",
            "200 391 Loss: 1819.246756\n",
            "250 391 Loss: 1818.858873\n",
            "300 391 Loss: 1818.669687\n",
            "350 391 Loss: 1818.777312\n",
            "156 157 ValLoss: 1821.071900\n",
            "\n",
            "Epoch: 36 2022-11-27 10:15:30.517396\n",
            "0 391 Loss: 1810.867310\n",
            "50 391 Loss: 1817.718841\n",
            "100 391 Loss: 1819.209659\n",
            "150 391 Loss: 1818.140159\n",
            "200 391 Loss: 1818.021340\n",
            "250 391 Loss: 1818.101018\n",
            "300 391 Loss: 1818.741118\n",
            "350 391 Loss: 1818.446012\n",
            "156 157 ValLoss: 1820.205440\n",
            "\n",
            "Epoch: 37 2022-11-27 10:15:43.179021\n",
            "0 391 Loss: 1824.922485\n",
            "50 391 Loss: 1819.664680\n",
            "100 391 Loss: 1819.971005\n",
            "150 391 Loss: 1818.395778\n",
            "200 391 Loss: 1818.462923\n",
            "250 391 Loss: 1818.696197\n",
            "300 391 Loss: 1818.222552\n",
            "350 391 Loss: 1818.169624\n",
            "156 157 ValLoss: 1820.793567\n",
            "\n",
            "Epoch: 38 2022-11-27 10:15:55.733008\n",
            "0 391 Loss: 1795.180664\n",
            "50 391 Loss: 1817.883729\n",
            "100 391 Loss: 1817.153528\n",
            "150 391 Loss: 1816.026672\n",
            "200 391 Loss: 1815.469544\n",
            "250 391 Loss: 1817.031244\n",
            "300 391 Loss: 1816.999226\n",
            "350 391 Loss: 1817.570626\n",
            "156 157 ValLoss: 1820.958093\n",
            "\n",
            "Epoch: 39 2022-11-27 10:16:08.258905\n",
            "0 391 Loss: 1825.460083\n",
            "50 391 Loss: 1817.267147\n",
            "100 391 Loss: 1813.485795\n",
            "150 391 Loss: 1815.313595\n",
            "200 391 Loss: 1816.009413\n",
            "250 391 Loss: 1817.237551\n",
            "300 391 Loss: 1817.767338\n",
            "350 391 Loss: 1818.133835\n",
            "156 157 ValLoss: 1820.397586\n",
            "\n",
            "Epoch: 40 2022-11-27 10:16:20.857718\n",
            "0 391 Loss: 1824.793457\n",
            "50 391 Loss: 1822.100028\n",
            "100 391 Loss: 1820.960942\n",
            "150 391 Loss: 1818.518019\n",
            "200 391 Loss: 1818.438311\n",
            "250 391 Loss: 1818.277636\n",
            "300 391 Loss: 1818.165017\n",
            "350 391 Loss: 1818.485104\n",
            "156 157 ValLoss: 1819.877934\n",
            "\n",
            "Epoch: 41 2022-11-27 10:16:33.434641\n",
            "0 391 Loss: 1801.558960\n",
            "50 391 Loss: 1818.090205\n",
            "100 391 Loss: 1819.747140\n",
            "150 391 Loss: 1820.322167\n",
            "200 391 Loss: 1819.227924\n",
            "250 391 Loss: 1818.825565\n",
            "300 391 Loss: 1818.375760\n",
            "350 391 Loss: 1817.829724\n",
            "156 157 ValLoss: 1820.246310\n",
            "\n",
            "Epoch: 42 2022-11-27 10:16:46.014792\n",
            "0 391 Loss: 1834.333008\n",
            "50 391 Loss: 1823.818625\n",
            "100 391 Loss: 1821.893821\n",
            "150 391 Loss: 1819.802595\n",
            "200 391 Loss: 1818.744241\n",
            "250 391 Loss: 1818.974490\n",
            "300 391 Loss: 1818.302037\n",
            "350 391 Loss: 1817.576095\n",
            "156 157 ValLoss: 1820.759197\n",
            "\n",
            "Epoch: 43 2022-11-27 10:16:58.547080\n",
            "0 391 Loss: 1844.916138\n",
            "50 391 Loss: 1816.724585\n",
            "100 391 Loss: 1814.636414\n",
            "150 391 Loss: 1816.272349\n",
            "200 391 Loss: 1816.516846\n",
            "250 391 Loss: 1817.612082\n",
            "300 391 Loss: 1818.011855\n",
            "350 391 Loss: 1818.552697\n",
            "156 157 ValLoss: 1820.069916\n",
            "\n",
            "Epoch: 44 2022-11-27 10:17:11.141222\n",
            "0 391 Loss: 1873.471924\n",
            "50 391 Loss: 1816.962812\n",
            "100 391 Loss: 1817.934959\n",
            "150 391 Loss: 1817.255679\n",
            "200 391 Loss: 1817.484670\n",
            "250 391 Loss: 1818.098968\n",
            "300 391 Loss: 1818.195521\n",
            "350 391 Loss: 1817.878470\n",
            "156 157 ValLoss: 1820.599951\n",
            "\n",
            "Epoch: 45 2022-11-27 10:17:23.704281\n",
            "0 391 Loss: 1843.131104\n",
            "50 391 Loss: 1814.123710\n",
            "100 391 Loss: 1816.559324\n",
            "150 391 Loss: 1818.130574\n",
            "200 391 Loss: 1817.857783\n",
            "250 391 Loss: 1818.449636\n",
            "300 391 Loss: 1817.418959\n",
            "350 391 Loss: 1817.428535\n",
            "156 157 ValLoss: 1822.484354\n",
            "\n",
            "Epoch: 46 2022-11-27 10:17:36.261195\n",
            "0 391 Loss: 1822.346313\n",
            "50 391 Loss: 1815.858451\n",
            "100 391 Loss: 1817.776835\n",
            "150 391 Loss: 1817.573423\n",
            "200 391 Loss: 1816.793711\n",
            "250 391 Loss: 1816.480355\n",
            "300 391 Loss: 1817.131691\n",
            "350 391 Loss: 1817.913599\n",
            "156 157 ValLoss: 1820.235306\n",
            "\n",
            "Epoch: 47 2022-11-27 10:17:48.853645\n",
            "0 391 Loss: 1797.933838\n",
            "50 391 Loss: 1816.432900\n",
            "100 391 Loss: 1815.581171\n",
            "150 391 Loss: 1817.219172\n",
            "200 391 Loss: 1816.790933\n",
            "250 391 Loss: 1817.026913\n",
            "300 391 Loss: 1817.587496\n",
            "350 391 Loss: 1817.514953\n",
            "156 157 ValLoss: 1820.325980\n",
            "\n",
            "Epoch: 48 2022-11-27 10:18:01.490161\n",
            "0 391 Loss: 1801.433350\n",
            "50 391 Loss: 1817.957862\n",
            "100 391 Loss: 1815.683294\n",
            "150 391 Loss: 1816.671025\n",
            "200 391 Loss: 1816.914464\n",
            "250 391 Loss: 1817.556638\n",
            "300 391 Loss: 1817.315678\n",
            "350 391 Loss: 1817.585950\n",
            "156 157 ValLoss: 1819.879636\n",
            "\n",
            "Epoch: 49 2022-11-27 10:18:14.032799\n",
            "0 391 Loss: 1759.922485\n",
            "50 391 Loss: 1820.473202\n",
            "100 391 Loss: 1819.282131\n",
            "150 391 Loss: 1816.385717\n",
            "200 391 Loss: 1816.772090\n",
            "250 391 Loss: 1816.626666\n",
            "300 391 Loss: 1817.048067\n",
            "350 391 Loss: 1817.213555\n",
            "156 157 ValLoss: 1819.764065\n",
            "\n",
            "Epoch: 50 2022-11-27 10:18:26.637702\n",
            "0 391 Loss: 1770.204346\n",
            "50 391 Loss: 1814.953367\n",
            "100 391 Loss: 1816.801732\n",
            "150 391 Loss: 1817.984525\n",
            "200 391 Loss: 1817.525009\n",
            "250 391 Loss: 1816.965156\n",
            "300 391 Loss: 1817.717038\n",
            "350 391 Loss: 1817.427342\n",
            "156 157 ValLoss: 1819.668065\n",
            "\n",
            "Epoch: 51 2022-11-27 10:18:39.119482\n",
            "0 391 Loss: 1832.920776\n",
            "50 391 Loss: 1817.981419\n",
            "100 391 Loss: 1817.693538\n",
            "150 391 Loss: 1819.498272\n",
            "200 391 Loss: 1818.866505\n",
            "250 391 Loss: 1818.667800\n",
            "300 391 Loss: 1817.545518\n",
            "350 391 Loss: 1817.579965\n",
            "156 157 ValLoss: 1820.275957\n",
            "\n",
            "Epoch: 52 2022-11-27 10:18:51.580129\n",
            "0 391 Loss: 1824.902832\n",
            "50 391 Loss: 1817.539709\n",
            "100 391 Loss: 1817.352071\n",
            "150 391 Loss: 1817.472116\n",
            "200 391 Loss: 1817.346465\n",
            "250 391 Loss: 1818.156297\n",
            "300 391 Loss: 1817.966456\n",
            "350 391 Loss: 1817.740032\n",
            "156 157 ValLoss: 1819.740012\n",
            "\n",
            "Epoch: 53 2022-11-27 10:19:04.109261\n",
            "0 391 Loss: 1824.642578\n",
            "50 391 Loss: 1818.552866\n",
            "100 391 Loss: 1818.385592\n",
            "150 391 Loss: 1816.721636\n",
            "200 391 Loss: 1816.914914\n",
            "250 391 Loss: 1817.155218\n",
            "300 391 Loss: 1817.565428\n",
            "350 391 Loss: 1817.520181\n",
            "156 157 ValLoss: 1819.823606\n",
            "\n",
            "Epoch: 54 2022-11-27 10:19:16.711633\n",
            "0 391 Loss: 1836.628784\n",
            "50 391 Loss: 1818.989342\n",
            "100 391 Loss: 1820.611947\n",
            "150 391 Loss: 1820.594589\n",
            "200 391 Loss: 1819.346755\n",
            "250 391 Loss: 1818.429450\n",
            "300 391 Loss: 1817.721918\n",
            "350 391 Loss: 1817.780869\n",
            "156 157 ValLoss: 1819.662046\n",
            "\n",
            "Epoch: 55 2022-11-27 10:19:29.308788\n",
            "0 391 Loss: 1812.868530\n",
            "50 391 Loss: 1818.758629\n",
            "100 391 Loss: 1818.402010\n",
            "150 391 Loss: 1818.995556\n",
            "200 391 Loss: 1818.916695\n",
            "250 391 Loss: 1817.587830\n",
            "300 391 Loss: 1817.703400\n",
            "350 391 Loss: 1817.618484\n",
            "156 157 ValLoss: 1819.681216\n",
            "\n",
            "Epoch: 56 2022-11-27 10:19:41.894474\n",
            "0 391 Loss: 1825.112183\n",
            "50 391 Loss: 1823.981579\n",
            "100 391 Loss: 1820.529044\n",
            "150 391 Loss: 1818.667894\n",
            "200 391 Loss: 1817.381242\n",
            "250 391 Loss: 1817.248005\n",
            "300 391 Loss: 1817.246863\n",
            "350 391 Loss: 1817.575678\n",
            "156 157 ValLoss: 1819.927375\n",
            "\n",
            "Epoch: 57 2022-11-27 10:19:54.445966\n",
            "0 391 Loss: 1803.064209\n",
            "50 391 Loss: 1813.774230\n",
            "100 391 Loss: 1814.792937\n",
            "150 391 Loss: 1815.536430\n",
            "200 391 Loss: 1816.205061\n",
            "250 391 Loss: 1817.203659\n",
            "300 391 Loss: 1817.541235\n",
            "350 391 Loss: 1817.638129\n",
            "156 157 ValLoss: 1820.092030\n",
            "\n",
            "Epoch: 58 2022-11-27 10:20:07.080705\n",
            "0 391 Loss: 1807.111694\n",
            "50 391 Loss: 1818.656475\n",
            "100 391 Loss: 1815.767034\n",
            "150 391 Loss: 1815.946077\n",
            "200 391 Loss: 1816.257246\n",
            "250 391 Loss: 1816.162951\n",
            "300 391 Loss: 1817.302639\n",
            "350 391 Loss: 1816.962773\n",
            "156 157 ValLoss: 1819.585051\n",
            "\n",
            "Epoch: 59 2022-11-27 10:20:19.546079\n",
            "0 391 Loss: 1821.587402\n",
            "50 391 Loss: 1816.290963\n",
            "100 391 Loss: 1818.512433\n",
            "150 391 Loss: 1817.138946\n",
            "200 391 Loss: 1816.360383\n",
            "250 391 Loss: 1817.199802\n",
            "300 391 Loss: 1817.082685\n",
            "350 391 Loss: 1817.264471\n",
            "156 157 ValLoss: 1819.366661\n",
            "\n",
            "Epoch: 60 2022-11-27 10:20:32.124037\n",
            "0 391 Loss: 1845.434937\n",
            "50 391 Loss: 1812.565305\n",
            "100 391 Loss: 1815.655948\n",
            "150 391 Loss: 1817.473886\n",
            "200 391 Loss: 1818.338610\n",
            "250 391 Loss: 1816.575308\n",
            "300 391 Loss: 1816.501034\n",
            "350 391 Loss: 1816.613176\n",
            "156 157 ValLoss: 1819.812178\n",
            "\n",
            "Epoch: 61 2022-11-27 10:20:44.732651\n",
            "0 391 Loss: 1784.484375\n",
            "50 391 Loss: 1818.807445\n",
            "100 391 Loss: 1818.153046\n",
            "150 391 Loss: 1817.663050\n",
            "200 391 Loss: 1817.850438\n",
            "250 391 Loss: 1817.191587\n",
            "300 391 Loss: 1817.216775\n",
            "350 391 Loss: 1817.305763\n",
            "156 157 ValLoss: 1819.908053\n",
            "\n",
            "Epoch: 62 2022-11-27 10:20:57.234329\n",
            "0 391 Loss: 1824.505859\n",
            "50 391 Loss: 1818.588087\n",
            "100 391 Loss: 1816.599068\n",
            "150 391 Loss: 1816.973620\n",
            "200 391 Loss: 1817.105149\n",
            "250 391 Loss: 1817.086146\n",
            "300 391 Loss: 1817.006184\n",
            "350 391 Loss: 1817.124220\n",
            "156 157 ValLoss: 1819.590472\n",
            "\n",
            "Epoch: 63 2022-11-27 10:21:09.723775\n",
            "0 391 Loss: 1840.567383\n",
            "50 391 Loss: 1824.509131\n",
            "100 391 Loss: 1818.294354\n",
            "150 391 Loss: 1817.275615\n",
            "200 391 Loss: 1817.810158\n",
            "250 391 Loss: 1816.891817\n",
            "300 391 Loss: 1816.824138\n",
            "350 391 Loss: 1817.063999\n",
            "156 157 ValLoss: 1819.465720\n",
            "\n",
            "Epoch: 64 2022-11-27 10:21:22.223555\n",
            "0 391 Loss: 1814.462402\n",
            "50 391 Loss: 1815.097625\n",
            "100 391 Loss: 1816.303050\n",
            "150 391 Loss: 1816.721163\n",
            "200 391 Loss: 1816.025523\n",
            "250 391 Loss: 1816.424156\n",
            "300 391 Loss: 1817.014115\n",
            "350 391 Loss: 1816.799585\n",
            "156 157 ValLoss: 1819.496955\n",
            "\n",
            "Epoch: 65 2022-11-27 10:21:34.773276\n",
            "0 391 Loss: 1809.674316\n",
            "50 391 Loss: 1816.221919\n",
            "100 391 Loss: 1819.084164\n",
            "150 391 Loss: 1816.781762\n",
            "200 391 Loss: 1816.632537\n",
            "250 391 Loss: 1817.570168\n",
            "300 391 Loss: 1817.294230\n",
            "350 391 Loss: 1817.111211\n",
            "156 157 ValLoss: 1819.379075\n",
            "\n",
            "Epoch: 66 2022-11-27 10:21:47.275275\n",
            "0 391 Loss: 1816.085693\n",
            "50 391 Loss: 1820.199147\n",
            "100 391 Loss: 1816.535661\n",
            "150 391 Loss: 1816.270647\n",
            "200 391 Loss: 1816.530051\n",
            "250 391 Loss: 1817.457365\n",
            "300 391 Loss: 1817.474385\n",
            "350 391 Loss: 1818.279032\n",
            "156 157 ValLoss: 1819.500506\n",
            "\n",
            "Epoch: 67 2022-11-27 10:21:59.796669\n",
            "0 391 Loss: 1806.505981\n",
            "50 391 Loss: 1816.700143\n",
            "100 391 Loss: 1817.604745\n",
            "150 391 Loss: 1817.619843\n",
            "200 391 Loss: 1817.184591\n",
            "250 391 Loss: 1817.710971\n",
            "300 391 Loss: 1817.066184\n",
            "350 391 Loss: 1817.083110\n",
            "156 157 ValLoss: 1819.252157\n",
            "\n",
            "Epoch: 68 2022-11-27 10:22:12.355185\n",
            "0 391 Loss: 1814.402588\n",
            "50 391 Loss: 1819.549527\n",
            "100 391 Loss: 1819.327917\n",
            "150 391 Loss: 1816.864217\n",
            "200 391 Loss: 1817.205308\n",
            "250 391 Loss: 1817.931494\n",
            "300 391 Loss: 1817.542960\n",
            "350 391 Loss: 1816.698068\n",
            "156 157 ValLoss: 1819.468814\n",
            "\n",
            "Epoch: 69 2022-11-27 10:22:24.949097\n",
            "0 391 Loss: 1835.854126\n",
            "50 391 Loss: 1818.741580\n",
            "100 391 Loss: 1816.617323\n",
            "150 391 Loss: 1818.964090\n",
            "200 391 Loss: 1817.371030\n",
            "250 391 Loss: 1817.668513\n",
            "300 391 Loss: 1816.911008\n",
            "350 391 Loss: 1816.852240\n",
            "156 157 ValLoss: 1819.636723\n",
            "\n",
            "Epoch: 70 2022-11-27 10:22:37.493740\n",
            "0 391 Loss: 1798.925415\n",
            "50 391 Loss: 1812.509009\n",
            "100 391 Loss: 1815.237899\n",
            "150 391 Loss: 1816.871615\n",
            "200 391 Loss: 1817.332271\n",
            "250 391 Loss: 1817.142328\n",
            "300 391 Loss: 1816.693404\n",
            "350 391 Loss: 1816.229077\n",
            "156 157 ValLoss: 1819.414270\n",
            "\n",
            "Epoch: 71 2022-11-27 10:22:50.000869\n",
            "0 391 Loss: 1827.772339\n",
            "50 391 Loss: 1816.236845\n",
            "100 391 Loss: 1818.034820\n",
            "150 391 Loss: 1816.861183\n",
            "200 391 Loss: 1817.173584\n",
            "250 391 Loss: 1816.935488\n",
            "300 391 Loss: 1817.006792\n",
            "350 391 Loss: 1816.975091\n",
            "156 157 ValLoss: 1819.038022\n",
            "\n",
            "Epoch: 72 2022-11-27 10:23:02.529393\n",
            "0 391 Loss: 1808.984253\n",
            "50 391 Loss: 1816.946481\n",
            "100 391 Loss: 1817.236237\n",
            "150 391 Loss: 1817.398716\n",
            "200 391 Loss: 1818.316180\n",
            "250 391 Loss: 1817.409422\n",
            "300 391 Loss: 1817.462968\n",
            "350 391 Loss: 1816.716725\n",
            "156 157 ValLoss: 1819.215651\n",
            "\n",
            "Epoch: 73 2022-11-27 10:23:15.032881\n",
            "0 391 Loss: 1791.932495\n",
            "50 391 Loss: 1821.863159\n",
            "100 391 Loss: 1818.638309\n",
            "150 391 Loss: 1816.978713\n",
            "200 391 Loss: 1817.139252\n",
            "250 391 Loss: 1816.949721\n",
            "300 391 Loss: 1816.366094\n",
            "350 391 Loss: 1816.308362\n",
            "156 157 ValLoss: 1819.509969\n",
            "\n",
            "Epoch: 74 2022-11-27 10:23:27.642609\n",
            "0 391 Loss: 1813.176025\n",
            "50 391 Loss: 1818.735380\n",
            "100 391 Loss: 1816.220969\n",
            "150 391 Loss: 1817.916277\n",
            "200 391 Loss: 1817.942211\n",
            "250 391 Loss: 1817.099669\n",
            "300 391 Loss: 1816.567138\n",
            "350 391 Loss: 1816.973267\n",
            "156 157 ValLoss: 1819.133934\n",
            "\n",
            "Epoch: 75 2022-11-27 10:23:40.177260\n",
            "0 391 Loss: 1794.826660\n",
            "50 391 Loss: 1817.955215\n",
            "100 391 Loss: 1814.972342\n",
            "150 391 Loss: 1814.118461\n",
            "200 391 Loss: 1814.585746\n",
            "250 391 Loss: 1815.267475\n",
            "300 391 Loss: 1816.900513\n",
            "350 391 Loss: 1816.530353\n",
            "156 157 ValLoss: 1820.149833\n",
            "\n",
            "Epoch: 76 2022-11-27 10:23:52.711503\n",
            "0 391 Loss: 1811.211670\n",
            "50 391 Loss: 1817.076890\n",
            "100 391 Loss: 1816.895302\n",
            "150 391 Loss: 1816.446372\n",
            "200 391 Loss: 1816.138632\n",
            "250 391 Loss: 1816.822842\n",
            "300 391 Loss: 1816.173970\n",
            "350 391 Loss: 1816.467708\n",
            "156 157 ValLoss: 1819.001134\n",
            "\n",
            "Epoch: 77 2022-11-27 10:24:05.233188\n",
            "0 391 Loss: 1806.956787\n",
            "50 391 Loss: 1815.504509\n",
            "100 391 Loss: 1816.667506\n",
            "150 391 Loss: 1816.533763\n",
            "200 391 Loss: 1817.141229\n",
            "250 391 Loss: 1817.065942\n",
            "300 391 Loss: 1817.152914\n",
            "350 391 Loss: 1816.742321\n",
            "156 157 ValLoss: 1819.189745\n",
            "\n",
            "Epoch: 78 2022-11-27 10:24:17.763416\n",
            "0 391 Loss: 1810.985596\n",
            "50 391 Loss: 1816.412342\n",
            "100 391 Loss: 1815.597636\n",
            "150 391 Loss: 1815.202182\n",
            "200 391 Loss: 1814.078931\n",
            "250 391 Loss: 1814.423381\n",
            "300 391 Loss: 1815.664472\n",
            "350 391 Loss: 1816.466522\n",
            "156 157 ValLoss: 1819.072394\n",
            "\n",
            "Epoch: 79 2022-11-27 10:24:30.339298\n",
            "0 391 Loss: 1761.231812\n",
            "50 391 Loss: 1813.483027\n",
            "100 391 Loss: 1815.761160\n",
            "150 391 Loss: 1816.771825\n",
            "200 391 Loss: 1817.011411\n",
            "250 391 Loss: 1816.598123\n",
            "300 391 Loss: 1816.282946\n",
            "350 391 Loss: 1816.422977\n",
            "156 157 ValLoss: 1818.944937\n",
            "\n",
            "Epoch: 80 2022-11-27 10:24:42.987517\n",
            "0 391 Loss: 1803.929565\n",
            "50 391 Loss: 1818.176205\n",
            "100 391 Loss: 1816.811446\n",
            "150 391 Loss: 1816.426800\n",
            "200 391 Loss: 1815.918871\n",
            "250 391 Loss: 1816.749861\n",
            "300 391 Loss: 1816.353961\n",
            "350 391 Loss: 1816.841775\n",
            "156 157 ValLoss: 1818.902399\n",
            "\n",
            "Epoch: 81 2022-11-27 10:24:55.651374\n",
            "0 391 Loss: 1804.647705\n",
            "50 391 Loss: 1814.086031\n",
            "100 391 Loss: 1815.615632\n",
            "150 391 Loss: 1815.726594\n",
            "200 391 Loss: 1816.532613\n",
            "250 391 Loss: 1816.323355\n",
            "300 391 Loss: 1816.149684\n",
            "350 391 Loss: 1816.247529\n",
            "156 157 ValLoss: 1819.138904\n",
            "\n",
            "Epoch: 82 2022-11-27 10:25:08.355651\n",
            "0 391 Loss: 1852.294556\n",
            "50 391 Loss: 1820.271010\n",
            "100 391 Loss: 1818.447521\n",
            "150 391 Loss: 1817.646872\n",
            "200 391 Loss: 1817.086977\n",
            "250 391 Loss: 1816.768438\n",
            "300 391 Loss: 1817.012250\n",
            "350 391 Loss: 1816.646204\n",
            "156 157 ValLoss: 1819.275697\n",
            "\n",
            "Epoch: 83 2022-11-27 10:25:21.034024\n",
            "0 391 Loss: 1813.151489\n",
            "50 391 Loss: 1820.250247\n",
            "100 391 Loss: 1819.418016\n",
            "150 391 Loss: 1818.012583\n",
            "200 391 Loss: 1816.456696\n",
            "250 391 Loss: 1816.358678\n",
            "300 391 Loss: 1816.956691\n",
            "350 391 Loss: 1816.819629\n",
            "156 157 ValLoss: 1819.323980\n",
            "\n",
            "Epoch: 84 2022-11-27 10:25:33.853637\n",
            "0 391 Loss: 1814.325195\n",
            "50 391 Loss: 1816.737805\n",
            "100 391 Loss: 1815.875554\n",
            "150 391 Loss: 1816.530435\n",
            "200 391 Loss: 1816.656204\n",
            "250 391 Loss: 1816.525141\n",
            "300 391 Loss: 1816.541923\n",
            "350 391 Loss: 1816.590964\n",
            "156 157 ValLoss: 1818.968557\n",
            "\n",
            "Epoch: 85 2022-11-27 10:25:46.482502\n",
            "0 391 Loss: 1822.649780\n",
            "50 391 Loss: 1812.430781\n",
            "100 391 Loss: 1812.036574\n",
            "150 391 Loss: 1814.424198\n",
            "200 391 Loss: 1815.677798\n",
            "250 391 Loss: 1815.479660\n",
            "300 391 Loss: 1815.935666\n",
            "350 391 Loss: 1816.849104\n",
            "156 157 ValLoss: 1819.087943\n",
            "\n",
            "Epoch: 86 2022-11-27 10:25:59.086990\n",
            "0 391 Loss: 1840.875732\n",
            "50 391 Loss: 1814.396420\n",
            "100 391 Loss: 1815.883659\n",
            "150 391 Loss: 1815.439101\n",
            "200 391 Loss: 1815.181092\n",
            "250 391 Loss: 1815.297994\n",
            "300 391 Loss: 1815.549799\n",
            "350 391 Loss: 1816.211889\n",
            "156 157 ValLoss: 1818.916049\n",
            "\n",
            "Epoch: 87 2022-11-27 10:26:11.796099\n",
            "0 391 Loss: 1802.432007\n",
            "50 391 Loss: 1814.232015\n",
            "100 391 Loss: 1813.761495\n",
            "150 391 Loss: 1815.120613\n",
            "200 391 Loss: 1814.655422\n",
            "250 391 Loss: 1815.759062\n",
            "300 391 Loss: 1815.981510\n",
            "350 391 Loss: 1815.870067\n",
            "156 157 ValLoss: 1819.144519\n",
            "\n",
            "Epoch: 88 2022-11-27 10:26:24.335256\n",
            "0 391 Loss: 1816.734863\n",
            "50 391 Loss: 1821.925666\n",
            "100 391 Loss: 1816.219934\n",
            "150 391 Loss: 1815.709873\n",
            "200 391 Loss: 1816.998429\n",
            "250 391 Loss: 1816.676217\n",
            "300 391 Loss: 1816.223927\n",
            "350 391 Loss: 1816.236885\n",
            "156 157 ValLoss: 1819.230711\n",
            "\n",
            "Epoch: 89 2022-11-27 10:26:36.865567\n",
            "0 391 Loss: 1835.502563\n",
            "50 391 Loss: 1812.174065\n",
            "100 391 Loss: 1812.914302\n",
            "150 391 Loss: 1814.736834\n",
            "200 391 Loss: 1814.601078\n",
            "250 391 Loss: 1815.621361\n",
            "300 391 Loss: 1815.659175\n",
            "350 391 Loss: 1816.207230\n",
            "156 157 ValLoss: 1818.814881\n",
            "\n",
            "Epoch: 90 2022-11-27 10:26:49.408845\n",
            "0 391 Loss: 1826.434570\n",
            "50 391 Loss: 1818.510819\n",
            "100 391 Loss: 1819.756559\n",
            "150 391 Loss: 1817.628654\n",
            "200 391 Loss: 1817.829998\n",
            "250 391 Loss: 1816.941006\n",
            "300 391 Loss: 1816.645217\n",
            "350 391 Loss: 1816.599888\n",
            "156 157 ValLoss: 1819.109942\n",
            "\n",
            "Epoch: 91 2022-11-27 10:27:01.885058\n",
            "0 391 Loss: 1844.661133\n",
            "50 391 Loss: 1819.615428\n",
            "100 391 Loss: 1817.809349\n",
            "150 391 Loss: 1818.774264\n",
            "200 391 Loss: 1817.763112\n",
            "250 391 Loss: 1816.889095\n",
            "300 391 Loss: 1816.565217\n",
            "350 391 Loss: 1816.514871\n",
            "156 157 ValLoss: 1819.037890\n",
            "\n",
            "Epoch: 92 2022-11-27 10:27:14.406994\n",
            "0 391 Loss: 1834.141846\n",
            "50 391 Loss: 1817.795039\n",
            "100 391 Loss: 1816.106424\n",
            "150 391 Loss: 1816.325351\n",
            "200 391 Loss: 1816.671828\n",
            "250 391 Loss: 1817.126626\n",
            "300 391 Loss: 1817.516314\n",
            "350 391 Loss: 1816.590360\n",
            "156 157 ValLoss: 1818.935269\n",
            "\n",
            "Epoch: 93 2022-11-27 10:27:26.969505\n",
            "0 391 Loss: 1780.912598\n",
            "50 391 Loss: 1813.941097\n",
            "100 391 Loss: 1816.201505\n",
            "150 391 Loss: 1818.776242\n",
            "200 391 Loss: 1817.032813\n",
            "250 391 Loss: 1816.063393\n",
            "300 391 Loss: 1817.195027\n",
            "350 391 Loss: 1817.100043\n",
            "156 157 ValLoss: 1819.970386\n",
            "\n",
            "Epoch: 94 2022-11-27 10:27:39.565995\n",
            "0 391 Loss: 1794.798218\n",
            "50 391 Loss: 1819.100725\n",
            "100 391 Loss: 1816.092353\n",
            "150 391 Loss: 1817.273875\n",
            "200 391 Loss: 1816.667788\n",
            "250 391 Loss: 1816.744493\n",
            "300 391 Loss: 1816.826553\n",
            "350 391 Loss: 1816.269867\n",
            "156 157 ValLoss: 1818.742462\n",
            "\n",
            "Epoch: 95 2022-11-27 10:27:52.174101\n",
            "0 391 Loss: 1802.443359\n",
            "50 391 Loss: 1816.940619\n",
            "100 391 Loss: 1817.409041\n",
            "150 391 Loss: 1817.650475\n",
            "200 391 Loss: 1816.929258\n",
            "250 391 Loss: 1816.130279\n",
            "300 391 Loss: 1816.872486\n",
            "350 391 Loss: 1816.622503\n",
            "156 157 ValLoss: 1819.216296\n",
            "\n",
            "Epoch: 96 2022-11-27 10:28:04.784788\n",
            "0 391 Loss: 1823.477783\n",
            "50 391 Loss: 1814.563120\n",
            "100 391 Loss: 1817.546998\n",
            "150 391 Loss: 1816.584343\n",
            "200 391 Loss: 1815.663637\n",
            "250 391 Loss: 1816.742901\n",
            "300 391 Loss: 1816.905695\n",
            "350 391 Loss: 1816.635388\n",
            "156 157 ValLoss: 1818.912529\n",
            "\n",
            "Epoch: 97 2022-11-27 10:28:17.410395\n",
            "0 391 Loss: 1809.927002\n",
            "50 391 Loss: 1815.445940\n",
            "100 391 Loss: 1814.934006\n",
            "150 391 Loss: 1815.213722\n",
            "200 391 Loss: 1815.501402\n",
            "250 391 Loss: 1815.407197\n",
            "300 391 Loss: 1816.205083\n",
            "350 391 Loss: 1816.184674\n",
            "156 157 ValLoss: 1818.931011\n",
            "\n",
            "Epoch: 98 2022-11-27 10:28:29.986966\n",
            "0 391 Loss: 1819.889160\n",
            "50 391 Loss: 1815.973843\n",
            "100 391 Loss: 1814.852751\n",
            "150 391 Loss: 1816.111353\n",
            "200 391 Loss: 1816.277792\n",
            "250 391 Loss: 1816.648075\n",
            "300 391 Loss: 1816.526814\n",
            "350 391 Loss: 1816.356832\n",
            "156 157 ValLoss: 1818.931885\n",
            "\n",
            "Epoch: 99 2022-11-27 10:28:42.652245\n",
            "0 391 Loss: 1842.173096\n",
            "50 391 Loss: 1816.855143\n",
            "100 391 Loss: 1818.089939\n",
            "150 391 Loss: 1816.351341\n",
            "200 391 Loss: 1815.951363\n",
            "250 391 Loss: 1816.048434\n",
            "300 391 Loss: 1816.452669\n",
            "350 391 Loss: 1816.171422\n",
            "156 157 ValLoss: 1818.665067\n",
            "\n",
            "Epoch: 100 2022-11-27 10:28:55.302407\n",
            "0 391 Loss: 1815.023560\n",
            "50 391 Loss: 1814.296911\n",
            "100 391 Loss: 1812.940957\n",
            "150 391 Loss: 1814.368628\n",
            "200 391 Loss: 1814.894049\n",
            "250 391 Loss: 1814.262354\n",
            "300 391 Loss: 1815.348191\n",
            "350 391 Loss: 1815.971014\n",
            "156 157 ValLoss: 1818.844222\n",
            "\n",
            "Epoch: 101 2022-11-27 10:29:07.942642\n",
            "0 391 Loss: 1833.914917\n",
            "50 391 Loss: 1816.250725\n",
            "100 391 Loss: 1815.071538\n",
            "150 391 Loss: 1815.449813\n",
            "200 391 Loss: 1814.997842\n",
            "250 391 Loss: 1816.016931\n",
            "300 391 Loss: 1816.540112\n",
            "350 391 Loss: 1815.984506\n",
            "156 157 ValLoss: 1818.805295\n",
            "\n",
            "Epoch: 102 2022-11-27 10:29:20.566712\n",
            "0 391 Loss: 1787.955688\n",
            "50 391 Loss: 1810.934137\n",
            "100 391 Loss: 1813.419380\n",
            "150 391 Loss: 1813.442579\n",
            "200 391 Loss: 1814.205473\n",
            "250 391 Loss: 1815.505096\n",
            "300 391 Loss: 1815.637336\n",
            "350 391 Loss: 1815.973068\n",
            "156 157 ValLoss: 1818.728953\n",
            "\n",
            "Epoch: 103 2022-11-27 10:29:33.105232\n",
            "0 391 Loss: 1809.529175\n",
            "50 391 Loss: 1814.722302\n",
            "100 391 Loss: 1816.400333\n",
            "150 391 Loss: 1815.009098\n",
            "200 391 Loss: 1816.437640\n",
            "250 391 Loss: 1815.327884\n",
            "300 391 Loss: 1815.491211\n",
            "350 391 Loss: 1815.509724\n",
            "156 157 ValLoss: 1818.744594\n",
            "\n",
            "Epoch: 104 2022-11-27 10:29:45.660873\n",
            "0 391 Loss: 1818.143311\n",
            "50 391 Loss: 1822.454394\n",
            "100 391 Loss: 1817.731462\n",
            "150 391 Loss: 1815.883986\n",
            "200 391 Loss: 1815.225161\n",
            "250 391 Loss: 1815.551251\n",
            "300 391 Loss: 1815.320841\n",
            "350 391 Loss: 1815.695721\n",
            "156 157 ValLoss: 1818.658503\n",
            "\n",
            "Epoch: 105 2022-11-27 10:29:58.187297\n",
            "0 391 Loss: 1799.808716\n",
            "50 391 Loss: 1816.388789\n",
            "100 391 Loss: 1817.848390\n",
            "150 391 Loss: 1816.702555\n",
            "200 391 Loss: 1816.560568\n",
            "250 391 Loss: 1817.374316\n",
            "300 391 Loss: 1816.757546\n",
            "350 391 Loss: 1816.309159\n",
            "156 157 ValLoss: 1818.783702\n",
            "\n",
            "Epoch: 106 2022-11-27 10:30:10.890063\n",
            "0 391 Loss: 1809.496582\n",
            "50 391 Loss: 1816.423335\n",
            "100 391 Loss: 1815.490651\n",
            "150 391 Loss: 1815.013677\n",
            "200 391 Loss: 1815.568281\n",
            "250 391 Loss: 1814.587254\n",
            "300 391 Loss: 1815.941380\n",
            "350 391 Loss: 1816.253984\n",
            "156 157 ValLoss: 1818.614562\n",
            "\n",
            "Epoch: 107 2022-11-27 10:30:23.443671\n",
            "0 391 Loss: 1834.664429\n",
            "50 391 Loss: 1815.859521\n",
            "100 391 Loss: 1818.189226\n",
            "150 391 Loss: 1815.052608\n",
            "200 391 Loss: 1815.616502\n",
            "250 391 Loss: 1816.659534\n",
            "300 391 Loss: 1816.085207\n",
            "350 391 Loss: 1816.427760\n",
            "156 157 ValLoss: 1818.675750\n",
            "\n",
            "Epoch: 108 2022-11-27 10:30:35.991003\n",
            "0 391 Loss: 1818.821899\n",
            "50 391 Loss: 1813.060111\n",
            "100 391 Loss: 1813.082465\n",
            "150 391 Loss: 1813.822066\n",
            "200 391 Loss: 1814.313033\n",
            "250 391 Loss: 1815.281358\n",
            "300 391 Loss: 1816.436819\n",
            "350 391 Loss: 1816.027147\n",
            "156 157 ValLoss: 1818.826542\n",
            "\n",
            "Epoch: 109 2022-11-27 10:30:48.546353\n",
            "0 391 Loss: 1846.041748\n",
            "50 391 Loss: 1816.279098\n",
            "100 391 Loss: 1813.668620\n",
            "150 391 Loss: 1817.021990\n",
            "200 391 Loss: 1816.053166\n",
            "250 391 Loss: 1815.943415\n",
            "300 391 Loss: 1815.371355\n",
            "350 391 Loss: 1815.786796\n",
            "156 157 ValLoss: 1818.493810\n",
            "\n",
            "Epoch: 110 2022-11-27 10:31:01.068544\n",
            "0 391 Loss: 1809.321899\n",
            "50 391 Loss: 1812.986941\n",
            "100 391 Loss: 1812.625625\n",
            "150 391 Loss: 1814.424374\n",
            "200 391 Loss: 1815.253595\n",
            "250 391 Loss: 1815.293444\n",
            "300 391 Loss: 1816.139996\n",
            "350 391 Loss: 1815.924027\n",
            "156 157 ValLoss: 1819.058431\n",
            "\n",
            "Epoch: 111 2022-11-27 10:31:13.663817\n",
            "0 391 Loss: 1827.224854\n",
            "50 391 Loss: 1815.244160\n",
            "100 391 Loss: 1816.105754\n",
            "150 391 Loss: 1815.620359\n",
            "200 391 Loss: 1815.786665\n",
            "250 391 Loss: 1815.413145\n",
            "300 391 Loss: 1815.823371\n",
            "350 391 Loss: 1815.457969\n",
            "156 157 ValLoss: 1818.801008\n",
            "\n",
            "Epoch: 112 2022-11-27 10:31:26.213862\n",
            "0 391 Loss: 1814.959717\n",
            "50 391 Loss: 1815.463549\n",
            "100 391 Loss: 1814.332935\n",
            "150 391 Loss: 1814.808981\n",
            "200 391 Loss: 1813.874039\n",
            "250 391 Loss: 1814.781015\n",
            "300 391 Loss: 1815.029138\n",
            "350 391 Loss: 1814.886447\n",
            "156 157 ValLoss: 1818.756281\n",
            "\n",
            "Epoch: 113 2022-11-27 10:31:38.738163\n",
            "0 391 Loss: 1824.621582\n",
            "50 391 Loss: 1811.987544\n",
            "100 391 Loss: 1812.845001\n",
            "150 391 Loss: 1816.611771\n",
            "200 391 Loss: 1817.225707\n",
            "250 391 Loss: 1816.943560\n",
            "300 391 Loss: 1816.144263\n",
            "350 391 Loss: 1815.861162\n",
            "156 157 ValLoss: 1818.506033\n",
            "\n",
            "Epoch: 114 2022-11-27 10:31:51.280141\n",
            "0 391 Loss: 1820.836914\n",
            "50 391 Loss: 1816.140692\n",
            "100 391 Loss: 1816.918132\n",
            "150 391 Loss: 1817.632252\n",
            "200 391 Loss: 1816.476454\n",
            "250 391 Loss: 1816.550921\n",
            "300 391 Loss: 1816.458771\n",
            "350 391 Loss: 1816.119294\n",
            "156 157 ValLoss: 1818.507872\n",
            "\n",
            "Epoch: 115 2022-11-27 10:32:03.799109\n",
            "0 391 Loss: 1838.492798\n",
            "50 391 Loss: 1812.995019\n",
            "100 391 Loss: 1813.898755\n",
            "150 391 Loss: 1815.390180\n",
            "200 391 Loss: 1814.304250\n",
            "250 391 Loss: 1815.067237\n",
            "300 391 Loss: 1815.869962\n",
            "350 391 Loss: 1816.192150\n",
            "156 157 ValLoss: 1818.520413\n",
            "\n",
            "Epoch: 116 2022-11-27 10:32:16.451582\n",
            "0 391 Loss: 1822.247559\n",
            "50 391 Loss: 1817.506386\n",
            "100 391 Loss: 1815.167777\n",
            "150 391 Loss: 1815.115689\n",
            "200 391 Loss: 1815.972860\n",
            "250 391 Loss: 1816.467231\n",
            "300 391 Loss: 1815.776606\n",
            "350 391 Loss: 1815.396507\n",
            "156 157 ValLoss: 1818.619085\n",
            "\n",
            "Epoch: 117 2022-11-27 10:32:29.010190\n",
            "0 391 Loss: 1806.792847\n",
            "50 391 Loss: 1815.405417\n",
            "100 391 Loss: 1813.635592\n",
            "150 391 Loss: 1815.147230\n",
            "200 391 Loss: 1814.900723\n",
            "250 391 Loss: 1815.429817\n",
            "300 391 Loss: 1815.642146\n",
            "350 391 Loss: 1816.236764\n",
            "156 157 ValLoss: 1818.869114\n",
            "\n",
            "Epoch: 118 2022-11-27 10:32:41.651594\n",
            "0 391 Loss: 1813.163330\n",
            "50 391 Loss: 1818.358145\n",
            "100 391 Loss: 1818.383711\n",
            "150 391 Loss: 1815.895138\n",
            "200 391 Loss: 1816.493899\n",
            "250 391 Loss: 1816.313661\n",
            "300 391 Loss: 1816.439633\n",
            "350 391 Loss: 1816.253438\n",
            "156 157 ValLoss: 1818.621018\n",
            "\n",
            "Epoch: 119 2022-11-27 10:32:54.246888\n",
            "0 391 Loss: 1816.187866\n",
            "50 391 Loss: 1820.714813\n",
            "100 391 Loss: 1816.798921\n",
            "150 391 Loss: 1816.488006\n",
            "200 391 Loss: 1815.595869\n",
            "250 391 Loss: 1815.496042\n",
            "300 391 Loss: 1815.935390\n",
            "350 391 Loss: 1815.813513\n",
            "156 157 ValLoss: 1818.594649\n",
            "\n",
            "Epoch: 120 2022-11-27 10:33:06.773091\n",
            "0 391 Loss: 1803.512451\n",
            "50 391 Loss: 1815.414697\n",
            "100 391 Loss: 1812.860515\n",
            "150 391 Loss: 1813.379781\n",
            "200 391 Loss: 1813.394167\n",
            "250 391 Loss: 1814.056977\n",
            "300 391 Loss: 1815.078272\n",
            "350 391 Loss: 1815.562374\n",
            "156 157 ValLoss: 1819.356707\n",
            "\n",
            "Epoch: 121 2022-11-27 10:33:19.341734\n",
            "0 391 Loss: 1823.068237\n",
            "50 391 Loss: 1814.543421\n",
            "100 391 Loss: 1815.414645\n",
            "150 391 Loss: 1814.976315\n",
            "200 391 Loss: 1817.307172\n",
            "250 391 Loss: 1816.840462\n",
            "300 391 Loss: 1815.993592\n",
            "350 391 Loss: 1815.621215\n",
            "156 157 ValLoss: 1818.920268\n",
            "\n",
            "Epoch: 122 2022-11-27 10:33:31.916099\n",
            "0 391 Loss: 1812.668091\n",
            "50 391 Loss: 1813.949975\n",
            "100 391 Loss: 1814.816702\n",
            "150 391 Loss: 1815.325511\n",
            "200 391 Loss: 1815.749741\n",
            "250 391 Loss: 1815.668800\n",
            "300 391 Loss: 1815.559300\n",
            "350 391 Loss: 1815.670782\n",
            "156 157 ValLoss: 1818.704969\n",
            "\n",
            "Epoch: 123 2022-11-27 10:33:44.512684\n",
            "0 391 Loss: 1809.638062\n",
            "50 391 Loss: 1817.474047\n",
            "100 391 Loss: 1816.573691\n",
            "150 391 Loss: 1815.954450\n",
            "200 391 Loss: 1816.637270\n",
            "250 391 Loss: 1815.909523\n",
            "300 391 Loss: 1815.657731\n",
            "350 391 Loss: 1815.622369\n",
            "156 157 ValLoss: 1818.744853\n",
            "\n",
            "Epoch: 124 2022-11-27 10:33:57.126581\n",
            "0 391 Loss: 1831.167236\n",
            "50 391 Loss: 1817.796504\n",
            "100 391 Loss: 1813.685648\n",
            "150 391 Loss: 1814.304527\n",
            "200 391 Loss: 1815.000119\n",
            "250 391 Loss: 1815.705238\n",
            "300 391 Loss: 1815.748336\n",
            "350 391 Loss: 1815.719736\n",
            "156 157 ValLoss: 1818.804846\n",
            "\n",
            "Epoch: 125 2022-11-27 10:34:09.764824\n",
            "0 391 Loss: 1840.276367\n",
            "50 391 Loss: 1808.576658\n",
            "100 391 Loss: 1810.898677\n",
            "150 391 Loss: 1811.737129\n",
            "200 391 Loss: 1812.524324\n",
            "250 391 Loss: 1813.059346\n",
            "300 391 Loss: 1813.774415\n",
            "350 391 Loss: 1814.825148\n",
            "156 157 ValLoss: 1818.819145\n",
            "\n",
            "Epoch: 126 2022-11-27 10:34:22.353726\n",
            "0 391 Loss: 1856.773560\n",
            "50 391 Loss: 1819.210155\n",
            "100 391 Loss: 1819.063328\n",
            "150 391 Loss: 1816.942754\n",
            "200 391 Loss: 1816.712333\n",
            "250 391 Loss: 1816.785682\n",
            "300 391 Loss: 1817.030923\n",
            "350 391 Loss: 1816.720372\n",
            "156 157 ValLoss: 1818.660886\n",
            "\n",
            "Epoch: 127 2022-11-27 10:34:34.895061\n",
            "0 391 Loss: 1807.735596\n",
            "50 391 Loss: 1814.388782\n",
            "100 391 Loss: 1816.651629\n",
            "150 391 Loss: 1817.298095\n",
            "200 391 Loss: 1815.843872\n",
            "250 391 Loss: 1816.052793\n",
            "300 391 Loss: 1815.973714\n",
            "350 391 Loss: 1815.616433\n",
            "156 157 ValLoss: 1818.479683\n",
            "\n",
            "Epoch: 128 2022-11-27 10:34:47.462132\n",
            "0 391 Loss: 1801.902100\n",
            "50 391 Loss: 1811.175228\n",
            "100 391 Loss: 1814.322542\n",
            "150 391 Loss: 1812.973476\n",
            "200 391 Loss: 1812.795637\n",
            "250 391 Loss: 1814.220919\n",
            "300 391 Loss: 1815.587370\n",
            "350 391 Loss: 1815.600429\n",
            "156 157 ValLoss: 1818.597470\n",
            "\n",
            "Epoch: 129 2022-11-27 10:34:59.968506\n",
            "0 391 Loss: 1806.309937\n",
            "50 391 Loss: 1814.920551\n",
            "100 391 Loss: 1814.566103\n",
            "150 391 Loss: 1816.497135\n",
            "200 391 Loss: 1815.555071\n",
            "250 391 Loss: 1815.882009\n",
            "300 391 Loss: 1816.018623\n",
            "350 391 Loss: 1815.920742\n",
            "156 157 ValLoss: 1818.458955\n",
            "\n",
            "Epoch: 130 2022-11-27 10:35:12.502510\n",
            "0 391 Loss: 1799.536865\n",
            "50 391 Loss: 1817.516338\n",
            "100 391 Loss: 1813.954105\n",
            "150 391 Loss: 1814.504164\n",
            "200 391 Loss: 1815.037194\n",
            "250 391 Loss: 1814.777892\n",
            "300 391 Loss: 1815.690473\n",
            "350 391 Loss: 1815.777674\n",
            "156 157 ValLoss: 1818.549942\n",
            "\n",
            "Epoch: 131 2022-11-27 10:35:25.098881\n",
            "0 391 Loss: 1818.480225\n",
            "50 391 Loss: 1810.186191\n",
            "100 391 Loss: 1814.062127\n",
            "150 391 Loss: 1815.432671\n",
            "200 391 Loss: 1817.085648\n",
            "250 391 Loss: 1816.436108\n",
            "300 391 Loss: 1816.144470\n",
            "350 391 Loss: 1816.238886\n",
            "156 157 ValLoss: 1818.615540\n",
            "\n",
            "Epoch: 132 2022-11-27 10:35:37.698332\n",
            "0 391 Loss: 1834.060547\n",
            "50 391 Loss: 1815.524326\n",
            "100 391 Loss: 1815.091859\n",
            "150 391 Loss: 1815.675689\n",
            "200 391 Loss: 1815.338998\n",
            "250 391 Loss: 1815.868783\n",
            "300 391 Loss: 1815.809360\n",
            "350 391 Loss: 1816.754464\n",
            "156 157 ValLoss: 1818.472421\n",
            "\n",
            "Epoch: 133 2022-11-27 10:35:50.250612\n",
            "0 391 Loss: 1825.652222\n",
            "50 391 Loss: 1814.099269\n",
            "100 391 Loss: 1815.848022\n",
            "150 391 Loss: 1814.760766\n",
            "200 391 Loss: 1814.393647\n",
            "250 391 Loss: 1815.037803\n",
            "300 391 Loss: 1815.412374\n",
            "350 391 Loss: 1815.614797\n",
            "156 157 ValLoss: 1818.468568\n",
            "\n",
            "Epoch: 134 2022-11-27 10:36:02.786469\n",
            "0 391 Loss: 1758.921265\n",
            "50 391 Loss: 1815.111596\n",
            "100 391 Loss: 1815.605773\n",
            "150 391 Loss: 1815.852819\n",
            "200 391 Loss: 1816.401929\n",
            "250 391 Loss: 1816.135678\n",
            "300 391 Loss: 1816.148234\n",
            "350 391 Loss: 1815.699746\n",
            "156 157 ValLoss: 1818.686007\n",
            "\n",
            "Epoch: 135 2022-11-27 10:36:15.389491\n",
            "0 391 Loss: 1824.609131\n",
            "50 391 Loss: 1812.765524\n",
            "100 391 Loss: 1812.845267\n",
            "150 391 Loss: 1814.921143\n",
            "200 391 Loss: 1815.295470\n",
            "250 391 Loss: 1815.105585\n",
            "300 391 Loss: 1815.058739\n",
            "350 391 Loss: 1815.483935\n",
            "156 157 ValLoss: 1818.736115\n",
            "\n",
            "Epoch: 136 2022-11-27 10:36:27.997769\n",
            "0 391 Loss: 1793.628418\n",
            "50 391 Loss: 1814.894273\n",
            "100 391 Loss: 1815.558918\n",
            "150 391 Loss: 1815.125123\n",
            "200 391 Loss: 1815.597819\n",
            "250 391 Loss: 1815.762121\n",
            "300 391 Loss: 1815.142678\n",
            "350 391 Loss: 1815.116058\n",
            "156 157 ValLoss: 1818.836030\n",
            "\n",
            "Epoch: 137 2022-11-27 10:36:40.668817\n",
            "0 391 Loss: 1795.628296\n",
            "50 391 Loss: 1819.210188\n",
            "100 391 Loss: 1815.736287\n",
            "150 391 Loss: 1815.693841\n",
            "200 391 Loss: 1816.219875\n",
            "250 391 Loss: 1815.991215\n",
            "300 391 Loss: 1815.617273\n",
            "350 391 Loss: 1815.826788\n",
            "156 157 ValLoss: 1818.494896\n",
            "\n",
            "Epoch: 138 2022-11-27 10:36:53.395202\n",
            "0 391 Loss: 1821.246948\n",
            "50 391 Loss: 1817.696248\n",
            "100 391 Loss: 1817.132651\n",
            "150 391 Loss: 1815.895457\n",
            "200 391 Loss: 1815.757485\n",
            "250 391 Loss: 1816.123043\n",
            "300 391 Loss: 1815.719630\n",
            "350 391 Loss: 1815.545898\n",
            "156 157 ValLoss: 1818.364791\n",
            "\n",
            "Epoch: 139 2022-11-27 10:37:05.922354\n",
            "0 391 Loss: 1788.996704\n",
            "50 391 Loss: 1809.740033\n",
            "100 391 Loss: 1813.422665\n",
            "150 391 Loss: 1814.431512\n",
            "200 391 Loss: 1815.485947\n",
            "250 391 Loss: 1816.247842\n",
            "300 391 Loss: 1816.370061\n",
            "350 391 Loss: 1815.825012\n",
            "156 157 ValLoss: 1818.508572\n",
            "\n",
            "Epoch: 140 2022-11-27 10:37:18.532860\n",
            "0 391 Loss: 1847.395996\n",
            "50 391 Loss: 1818.244583\n",
            "100 391 Loss: 1815.002335\n",
            "150 391 Loss: 1814.351297\n",
            "200 391 Loss: 1814.518832\n",
            "250 391 Loss: 1815.287743\n",
            "300 391 Loss: 1814.602708\n",
            "350 391 Loss: 1815.060812\n",
            "156 157 ValLoss: 1818.558713\n",
            "\n",
            "Epoch: 141 2022-11-27 10:37:31.101229\n",
            "0 391 Loss: 1811.003174\n",
            "50 391 Loss: 1817.178852\n",
            "100 391 Loss: 1817.347634\n",
            "150 391 Loss: 1817.577505\n",
            "200 391 Loss: 1815.973740\n",
            "250 391 Loss: 1816.896947\n",
            "300 391 Loss: 1816.391798\n",
            "350 391 Loss: 1816.287260\n",
            "156 157 ValLoss: 1818.578693\n",
            "\n",
            "Epoch: 142 2022-11-27 10:37:43.644643\n",
            "0 391 Loss: 1827.171753\n",
            "50 391 Loss: 1815.548416\n",
            "100 391 Loss: 1816.833438\n",
            "150 391 Loss: 1815.412288\n",
            "200 391 Loss: 1814.894541\n",
            "250 391 Loss: 1816.333776\n",
            "300 391 Loss: 1815.683354\n",
            "350 391 Loss: 1815.345450\n",
            "156 157 ValLoss: 1818.606542\n",
            "\n",
            "Epoch: 143 2022-11-27 10:37:56.223107\n",
            "0 391 Loss: 1817.437012\n",
            "50 391 Loss: 1814.332175\n",
            "100 391 Loss: 1814.430275\n",
            "150 391 Loss: 1815.031919\n",
            "200 391 Loss: 1815.254418\n",
            "250 391 Loss: 1815.338273\n",
            "300 391 Loss: 1816.560481\n",
            "350 391 Loss: 1815.713706\n",
            "156 157 ValLoss: 1818.862951\n",
            "\n",
            "Epoch: 144 2022-11-27 10:38:08.790750\n",
            "0 391 Loss: 1784.234375\n",
            "50 391 Loss: 1815.665539\n",
            "100 391 Loss: 1813.781688\n",
            "150 391 Loss: 1813.172216\n",
            "200 391 Loss: 1814.247863\n",
            "250 391 Loss: 1815.325486\n",
            "300 391 Loss: 1816.081846\n",
            "350 391 Loss: 1815.302458\n",
            "156 157 ValLoss: 1818.828316\n",
            "\n",
            "Epoch: 145 2022-11-27 10:38:21.418139\n",
            "0 391 Loss: 1815.567505\n",
            "50 391 Loss: 1819.613784\n",
            "100 391 Loss: 1817.861996\n",
            "150 391 Loss: 1816.590898\n",
            "200 391 Loss: 1815.612904\n",
            "250 391 Loss: 1816.332627\n",
            "300 391 Loss: 1815.116252\n",
            "350 391 Loss: 1815.629949\n",
            "156 157 ValLoss: 1818.563532\n",
            "\n",
            "Epoch: 146 2022-11-27 10:38:33.966862\n",
            "0 391 Loss: 1842.825928\n",
            "50 391 Loss: 1815.027619\n",
            "100 391 Loss: 1814.387354\n",
            "150 391 Loss: 1814.480866\n",
            "200 391 Loss: 1815.488170\n",
            "250 391 Loss: 1815.078991\n",
            "300 391 Loss: 1815.019987\n",
            "350 391 Loss: 1815.125834\n",
            "156 157 ValLoss: 1818.419924\n",
            "\n",
            "Epoch: 147 2022-11-27 10:38:46.454755\n",
            "0 391 Loss: 1778.649536\n",
            "50 391 Loss: 1812.211356\n",
            "100 391 Loss: 1814.439604\n",
            "150 391 Loss: 1814.846854\n",
            "200 391 Loss: 1814.936647\n",
            "250 391 Loss: 1814.890188\n",
            "300 391 Loss: 1814.887186\n",
            "350 391 Loss: 1815.189139\n",
            "156 157 ValLoss: 1818.305700\n",
            "\n",
            "Epoch: 148 2022-11-27 10:38:58.992231\n",
            "0 391 Loss: 1804.274780\n",
            "50 391 Loss: 1810.877327\n",
            "100 391 Loss: 1811.858252\n",
            "150 391 Loss: 1815.181584\n",
            "200 391 Loss: 1814.708101\n",
            "250 391 Loss: 1815.720735\n",
            "300 391 Loss: 1815.674000\n",
            "350 391 Loss: 1815.805389\n",
            "156 157 ValLoss: 1818.445747\n",
            "\n",
            "Epoch: 149 2022-11-27 10:39:11.519031\n",
            "0 391 Loss: 1831.555908\n",
            "50 391 Loss: 1812.734528\n",
            "100 391 Loss: 1814.813335\n",
            "150 391 Loss: 1815.393080\n",
            "200 391 Loss: 1814.933795\n",
            "250 391 Loss: 1815.659868\n",
            "300 391 Loss: 1815.890105\n",
            "350 391 Loss: 1815.842557\n",
            "156 157 ValLoss: 1818.528818\n",
            "\n",
            "Epoch: 150 2022-11-27 10:39:24.138131\n",
            "0 391 Loss: 1819.655151\n",
            "50 391 Loss: 1816.401891\n",
            "100 391 Loss: 1815.665831\n",
            "150 391 Loss: 1816.818296\n",
            "200 391 Loss: 1815.672066\n",
            "250 391 Loss: 1815.223784\n",
            "300 391 Loss: 1814.886169\n",
            "350 391 Loss: 1815.235610\n",
            "156 157 ValLoss: 1818.469989\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.datasets import MNIST, FashionMNIST, CIFAR10, STL10\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "import google.colab\n",
        "import googleapiclient.discovery\n",
        "import googleapiclient.http\n",
        "\n",
        "class EncoderModule(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, stride, kernel, pad):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=kernel, padding=pad, stride=stride)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, color_channels, pooling_kernels, n_neurons_in_middle_layer):\n",
        "        self.n_neurons_in_middle_layer = n_neurons_in_middle_layer\n",
        "        super().__init__()\n",
        "        self.bottle = EncoderModule(color_channels, 32, stride=1, kernel=1, pad=0)\n",
        "        self.m1 = EncoderModule(32, 64, stride=1, kernel=3, pad=1)\n",
        "        self.m2 = EncoderModule(64, 128, stride=pooling_kernels[0], kernel=3, pad=1)\n",
        "        self.m3 = EncoderModule(128, 256, stride=pooling_kernels[1], kernel=3, pad=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.m3(self.m2(self.m1(self.bottle(x))))\n",
        "        return out.view(-1, self.n_neurons_in_middle_layer)\n",
        "        \n",
        "class DecoderModule(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, stride, activation=\"relu\"):\n",
        "        super().__init__()\n",
        "        self.convt = nn.ConvTranspose2d(input_channels, output_channels, kernel_size=stride, stride=stride)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        if activation == \"relu\":\n",
        "            self.activation = nn.ReLU(inplace=True)\n",
        "        elif activation == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.bn(self.convt(x)))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, color_channels, pooling_kernels, decoder_input_size):\n",
        "        self.decoder_input_size = decoder_input_size\n",
        "        super().__init__()\n",
        "        self.m1 = DecoderModule(256, 128, stride=1)\n",
        "        self.m2 = DecoderModule(128, 64, stride=pooling_kernels[1])\n",
        "        self.m3 = DecoderModule(64, 32, stride=pooling_kernels[0])\n",
        "        self.bottle = DecoderModule(32, color_channels, stride=1, activation=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x.view(-1, 256, self.decoder_input_size, self.decoder_input_size)\n",
        "        out = self.m3(self.m2(self.m1(out)))\n",
        "        return self.bottle(out)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        assert dataset in [\"mnist\" ,\"fashion-mnist\", \"cifar\", \"stl\"]\n",
        "\n",
        "        super().__init__()\n",
        "        # # latent features\n",
        "        self.n_latent_features = 64\n",
        "\n",
        "        # resolution\n",
        "        # mnist, fashion-mnist : 28 -> 14 -> 7\n",
        "        # cifar : 32 -> 8 -> 4\n",
        "        # stl : 96 -> 24 -> 6\n",
        "        if dataset in [\"mnist\", \"fashion-mnist\"]:\n",
        "            pooling_kernel = [2, 2]\n",
        "            encoder_output_size = 7\n",
        "        elif dataset == \"cifar\":\n",
        "            pooling_kernel = [4, 2]\n",
        "            encoder_output_size = 4\n",
        "        elif dataset == \"stl\":\n",
        "            pooling_kernel = [4, 4]\n",
        "            encoder_output_size = 6\n",
        "\n",
        "        # color channels\n",
        "        if dataset in [\"mnist\", \"fashion-mnist\"]:\n",
        "            color_channels = 1\n",
        "        else:\n",
        "            color_channels = 3\n",
        "\n",
        "        # # neurons int middle layer\n",
        "        n_neurons_middle_layer = 256 * encoder_output_size * encoder_output_size\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(color_channels, pooling_kernel, n_neurons_middle_layer)\n",
        "        # Middle\n",
        "        self.fc1 = nn.Linear(n_neurons_middle_layer, self.n_latent_features)\n",
        "        self.fc2 = nn.Linear(n_neurons_middle_layer, self.n_latent_features)\n",
        "        self.fc3 = nn.Linear(self.n_latent_features, n_neurons_middle_layer)\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(color_channels, pooling_kernel, encoder_output_size)\n",
        "\n",
        "        # data\n",
        "        self.train_loader, self.test_loader = self.load_data(dataset)\n",
        "        # history\n",
        "        self.history = {\"loss\":[], \"val_loss\":[]}\n",
        "\n",
        "        # model name\n",
        "        self.model_name = dataset\n",
        "        if not os.path.exists(self.model_name):\n",
        "            os.mkdir(self.model_name)\n",
        "\n",
        "    def _reparameterize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        esp = torch.randn(*mu.size()).to(self.device)\n",
        "        z = mu + std * esp\n",
        "        return z\n",
        "    \n",
        "    def _bottleneck(self, h):\n",
        "        mu, logvar = self.fc1(h), self.fc2(h)\n",
        "        z = self._reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "        \n",
        "    def sampling(self):\n",
        "        # assume latent features space ~ N(0, 1)\n",
        "        z = torch.randn(64, self.n_latent_features).to(self.device)\n",
        "        z = self.fc3(z)\n",
        "        # decode\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        h = self.encoder(x)\n",
        "        # Bottle-neck\n",
        "        z, mu, logvar = self._bottleneck(h)\n",
        "        # decoder\n",
        "        z = self.fc3(z)\n",
        "        d = self.decoder(z)\n",
        "        return d, mu, logvar\n",
        "\n",
        "    # Data\n",
        "    def load_data(self, dataset):\n",
        "        data_transform = transforms.Compose([\n",
        "                transforms.ToTensor()\n",
        "        ])\n",
        "        if dataset == \"mnist\":\n",
        "            train = MNIST(root=\"./data\", train=True, transform=data_transform, download=True)\n",
        "            test = MNIST(root=\"./data\", train=False, transform=data_transform, download=True)\n",
        "        elif dataset == \"fashion-mnist\":\n",
        "            train = FashionMNIST(root=\"./data\", train=True, transform=data_transform, download=True)\n",
        "            test = FashionMNIST(root=\"./data\", train=False, transform=data_transform, download=True)\n",
        "        elif dataset == \"cifar\":\n",
        "            train = CIFAR10(root=\"./data\", train=True, transform=data_transform, download=True)\n",
        "            test = CIFAR10(root=\"./data\", train=False, transform=data_transform, download=True)\n",
        "        elif dataset == \"stl\":\n",
        "            train = STL10(root=\"./data\", split=\"unlabeled\", transform=data_transform, download=True)\n",
        "            test = STL10(root=\"./data\", split=\"test\", transform=data_transform, download=True)\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=0)\n",
        "        test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True, num_workers=0)\n",
        "\n",
        "        return train_loader, test_loader\n",
        "\n",
        "    # Model\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
        "        BCE = F.binary_cross_entropy(recon_x, x, size_average=False)        \n",
        "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "        return BCE + KLD\n",
        "\n",
        "    def init_model(self):\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "        if self.device == \"cuda\":\n",
        "            self = self.cuda()\n",
        "            torch.backends.cudnn.benchmark=True\n",
        "        self.to(self.device)\n",
        "\n",
        "    # Train\n",
        "    def fit_train(self, epoch):\n",
        "        self.train()\n",
        "        print(f\"\\nEpoch: {epoch+1:d} {datetime.datetime.now()}\")\n",
        "        train_loss = 0\n",
        "        samples_cnt = 0\n",
        "        for batch_idx, (inputs, _) in enumerate(self.train_loader):\n",
        "            inputs = inputs.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar = self(inputs)\n",
        "\n",
        "            loss = self.loss_function(recon_batch, inputs, mu, logvar)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            samples_cnt += inputs.size(0)\n",
        "\n",
        "            if batch_idx%50 == 0:\n",
        "                print(batch_idx, len(self.train_loader), f\"Loss: {train_loss/samples_cnt:f}\")\n",
        "\n",
        "        self.history[\"loss\"].append(train_loss/samples_cnt)\n",
        "\n",
        "    def test(self, epoch):\n",
        "        self.eval()\n",
        "        val_loss = 0\n",
        "        samples_cnt = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, _) in enumerate(self.test_loader):\n",
        "                inputs = inputs.to(self.device)\n",
        "                recon_batch, mu, logvar = self(inputs)\n",
        "                val_loss += self.loss_function(recon_batch, inputs, mu, logvar).item()\n",
        "                samples_cnt += inputs.size(0)\n",
        "\n",
        "                if batch_idx == 0:\n",
        "                    save_image(recon_batch, f\"{self.model_name}/reconstruction_epoch_{str(epoch)}.png\", nrow=8)\n",
        "\n",
        "        print(batch_idx, len(self.test_loader), f\"ValLoss: {val_loss/samples_cnt:f}\")\n",
        "        self.history[\"val_loss\"].append(val_loss/samples_cnt)\n",
        "\n",
        "        # sampling\n",
        "        save_image(self.sampling(), f\"{self.model_name}/sampling_epoch_{str(epoch)}.png\", nrow=8)\n",
        "\n",
        "    # save results\n",
        "    def save_history(self):\n",
        "        with open(f\"{self.model_name}/{self.model_name}_history.dat\", \"wb\") as fp:\n",
        "            pickle.dump(self.history, fp)\n",
        "\n",
        "    def save_to_zip(self):\n",
        "        with zipfile.ZipFile(f\"{self.model_name}.zip\", \"w\") as zip:\n",
        "            for file in os.listdir(self.model_name):\n",
        "                zip.write(f\"{self.model_name}/{file}\", file)\n",
        "\n",
        "    def save_to_googledrive(self, drive_service):\n",
        "        saving_filename = self.model_name+\".zip\"\n",
        "\n",
        "        file_metadata = {\n",
        "          'name': saving_filename,\n",
        "          'mimeType': 'application/octet-stream'\n",
        "        }\n",
        "        media = googleapiclient.http.MediaFileUpload(saving_filename, \n",
        "                                mimetype='application/octet-stream',\n",
        "                                resumable=True)\n",
        "        created = drive_service.files().create(body=file_metadata,\n",
        "                                               media_body=media,\n",
        "                                               fields='id').execute()\n",
        "\n",
        "def google_drive_init():\n",
        "    google.colab.auth.authenticate_user()\n",
        "    return googleapiclient.discovery.build('drive', 'v3')\n",
        "\n",
        "def main():\n",
        "    googleclient = google_drive_init()\n",
        "    net = VAE(\"cifar\")\n",
        "    net.init_model()\n",
        "    for i in range(150):\n",
        "        net.fit_train(i)\n",
        "        net.test(i)\n",
        "    net.save_history()\n",
        "    net.save_to_zip()\n",
        "    net.save_to_googledrive(googleclient)\n",
        " \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}